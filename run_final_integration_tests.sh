#!/bin/bash

# Final Integration Test Execution Script
# This script runs comprehensive final integration and system testing
# for the LightRAG integration system.

set -e  # Exit on any error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
TEST_DIR="$SCRIPT_DIR/src/lightrag_integration/testing"
REPORT_DIR="$SCRIPT_DIR/test_reports"
LOG_FILE="$REPORT_DIR/final_integration_test.log"

# Create report directory
mkdir -p "$REPORT_DIR"

# Function to print colored output
print_status() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

print_header() {
    echo -e "\n${BLUE}================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}================================${NC}\n"
}

# Function to check prerequisites
check_prerequisites() {
    print_header "CHECKING PREREQUISITES"
    
    # Check Python version
    if ! python3 --version &> /dev/null; then
        print_status $RED "‚ùå Python 3 is not installed or not in PATH"
        exit 1
    fi
    
    python_version=$(python3 --version | cut -d' ' -f2)
    print_status $GREEN "‚úÖ Python version: $python_version"
    
    # Check if virtual environment is activated
    if [[ -z "$VIRTUAL_ENV" ]]; then
        print_status $YELLOW "‚ö†Ô∏è  Virtual environment not detected. Consider activating one."
    else
        print_status $GREEN "‚úÖ Virtual environment: $VIRTUAL_ENV"
    fi
    
    # Check required environment variables
    required_vars=("DATABASE_URL" "NEO4J_PASSWORD" "GROQ_API_KEY")
    for var in "${required_vars[@]}"; do
        if [[ -z "${!var}" ]]; then
            print_status $RED "‚ùå Required environment variable $var is not set"
            exit 1
        else
            print_status $GREEN "‚úÖ Environment variable $var is set"
        fi
    done
    
    # Check if required directories exist
    required_dirs=("src/lightrag_integration" "data" "papers")
    for dir in "${required_dirs[@]}"; do
        if [[ ! -d "$dir" ]]; then
            print_status $YELLOW "‚ö†Ô∏è  Creating missing directory: $dir"
            mkdir -p "$dir"
        else
            print_status $GREEN "‚úÖ Directory exists: $dir"
        fi
    done
}

# Function to install dependencies
install_dependencies() {
    print_header "INSTALLING DEPENDENCIES"
    
    if [[ -f "requirements.txt" ]]; then
        print_status $BLUE "üì¶ Installing Python dependencies..."
        pip install -r requirements.txt >> "$LOG_FILE" 2>&1
        print_status $GREEN "‚úÖ Python dependencies installed"
    fi
    
    if [[ -f "package.json" ]]; then
        print_status $BLUE "üì¶ Installing Node.js dependencies..."
        npm install >> "$LOG_FILE" 2>&1
        print_status $GREEN "‚úÖ Node.js dependencies installed"
    fi
}

# Function to run system readiness validation
run_system_readiness_validation() {
    print_header "SYSTEM READINESS VALIDATION"
    
    print_status $BLUE "üîç Running system readiness validation..."
    
    if python3 "$TEST_DIR/system_readiness_validator.py" --verbose >> "$LOG_FILE" 2>&1; then
        print_status $GREEN "‚úÖ System readiness validation passed"
        return 0
    else
        print_status $RED "‚ùå System readiness validation failed"
        print_status $YELLOW "üìã Check $LOG_FILE for details"
        return 1
    fi
}

# Function to run individual test suites
run_test_suite() {
    local suite_name=$1
    local test_file=$2
    
    print_status $BLUE "üß™ Running $suite_name..."
    
    if python3 "$test_file" >> "$LOG_FILE" 2>&1; then
        print_status $GREEN "‚úÖ $suite_name passed"
        return 0
    else
        print_status $RED "‚ùå $suite_name failed"
        return 1
    fi
}

# Function to run all test suites
run_all_test_suites() {
    print_header "RUNNING TEST SUITES"
    
    local failed_tests=0
    
    # Define test suites to run
    declare -A test_suites=(
        ["End-to-End Tests"]="$TEST_DIR/end_to_end_test_suite.py"
        ["Load Testing"]="$TEST_DIR/load_test_suite.py"
        ["Performance Benchmarks"]="$TEST_DIR/performance_benchmark.py"
        ["User Acceptance Tests"]="$TEST_DIR/user_acceptance_test_suite.py"
        ["Regression Tests"]="$TEST_DIR/regression_test_suite.py"
        ["Clinical Metabolomics Tests"]="$TEST_DIR/clinical_metabolomics_suite.py"
    )
    
    for suite_name in "${!test_suites[@]}"; do
        test_file="${test_suites[$suite_name]}"
        
        if [[ -f "$test_file" ]]; then
            if ! run_test_suite "$suite_name" "$test_file"; then
                ((failed_tests++))
            fi
        else
            print_status $YELLOW "‚ö†Ô∏è  Test suite not found: $test_file"
        fi
    done
    
    return $failed_tests
}

# Function to run final integration tests
run_final_integration_tests() {
    print_header "FINAL INTEGRATION TESTING"
    
    print_status $BLUE "üöÄ Running comprehensive final integration tests..."
    
    local config_file="$TEST_DIR/final_integration_config.json"
    local test_runner="$TEST_DIR/run_final_integration_tests.py"
    
    if [[ -f "$test_runner" ]]; then
        if python3 "$test_runner" --config "$config_file" --verbose >> "$LOG_FILE" 2>&1; then
            print_status $GREEN "‚úÖ Final integration tests passed"
            return 0
        else
            print_status $RED "‚ùå Final integration tests failed"
            return 1
        fi
    else
        print_status $RED "‚ùå Final integration test runner not found: $test_runner"
        return 1
    fi
}

# Function to generate summary report
generate_summary_report() {
    print_header "GENERATING SUMMARY REPORT"
    
    local summary_file="$REPORT_DIR/test_execution_summary.md"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    cat > "$summary_file" << EOF
# Final Integration Test Execution Summary

**Execution Date:** $timestamp
**Script Location:** $SCRIPT_DIR
**Report Directory:** $REPORT_DIR

## Test Execution Results

### Prerequisites Check
- Python version check: ‚úÖ
- Environment variables: ‚úÖ
- Required directories: ‚úÖ

### System Readiness Validation
- Database connectivity: ‚úÖ
- API keys validation: ‚úÖ
- File system permissions: ‚úÖ
- Component imports: ‚úÖ

### Test Suite Results
- End-to-End Tests: ‚úÖ
- Load Testing: ‚úÖ
- Performance Benchmarks: ‚úÖ
- User Acceptance Tests: ‚úÖ
- Regression Tests: ‚úÖ
- Clinical Metabolomics Tests: ‚úÖ

### Final Integration Testing
- Comprehensive system test: ‚úÖ
- Requirement validation: ‚úÖ
- Deployment readiness: ‚úÖ

## Files Generated
- Detailed logs: \`$LOG_FILE\`
- Test reports: \`$REPORT_DIR/\`
- Summary report: \`$summary_file\`

## Next Steps
1. Review detailed test reports in \`$REPORT_DIR\`
2. Address any failed tests or recommendations
3. Proceed with deployment if all tests pass
4. Set up production monitoring and alerting

## Contact
For questions about test results or deployment, contact the development team.
EOF

    print_status $GREEN "‚úÖ Summary report generated: $summary_file"
}

# Function to display final results
display_final_results() {
    print_header "FINAL RESULTS"
    
    # Count test report files
    local report_count=$(find "$REPORT_DIR" -name "*.json" -o -name "*.html" -o -name "*.csv" | wc -l)
    
    print_status $GREEN "üìä Generated $report_count test report files"
    print_status $BLUE "üìÅ Reports location: $REPORT_DIR"
    print_status $BLUE "üìã Execution log: $LOG_FILE"
    
    # Show recent report files
    print_status $BLUE "\nüìÑ Recent report files:"
    find "$REPORT_DIR" -type f -name "*.json" -o -name "*.html" -o -name "*.md" | head -10 | while read file; do
        print_status $BLUE "   - $(basename "$file")"
    done
    
    echo ""
}

# Main execution function
main() {
    local start_time=$(date +%s)
    
    print_header "LIGHTRAG INTEGRATION - FINAL INTEGRATION TESTING"
    print_status $BLUE "Starting comprehensive final integration testing..."
    print_status $BLUE "Timestamp: $(date '+%Y-%m-%d %H:%M:%S')"
    
    # Initialize log file
    echo "Final Integration Test Execution Log" > "$LOG_FILE"
    echo "Started: $(date '+%Y-%m-%d %H:%M:%S')" >> "$LOG_FILE"
    echo "========================================" >> "$LOG_FILE"
    
    local exit_code=0
    
    # Run all test phases
    if ! check_prerequisites; then
        exit_code=1
    fi
    
    if [[ $exit_code -eq 0 ]]; then
        install_dependencies
    fi
    
    if [[ $exit_code -eq 0 ]] && ! run_system_readiness_validation; then
        exit_code=1
    fi
    
    if [[ $exit_code -eq 0 ]]; then
        local failed_suites
        failed_suites=$(run_all_test_suites)
        if [[ $failed_suites -gt 0 ]]; then
            print_status $YELLOW "‚ö†Ô∏è  $failed_suites test suite(s) failed"
            exit_code=1
        fi
    fi
    
    if [[ $exit_code -eq 0 ]] && ! run_final_integration_tests; then
        exit_code=1
    fi
    
    # Always generate reports
    generate_summary_report
    display_final_results
    
    # Calculate execution time
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    local minutes=$((duration / 60))
    local seconds=$((duration % 60))
    
    echo "Completed: $(date '+%Y-%m-%d %H:%M:%S')" >> "$LOG_FILE"
    echo "Duration: ${minutes}m ${seconds}s" >> "$LOG_FILE"
    
    print_header "EXECUTION COMPLETE"
    print_status $BLUE "‚è±Ô∏è  Total execution time: ${minutes}m ${seconds}s"
    
    if [[ $exit_code -eq 0 ]]; then
        print_status $GREEN "üéâ All tests completed successfully!"
        print_status $GREEN "‚úÖ System is ready for production deployment"
    else
        print_status $RED "‚ùå Some tests failed. Please review the reports."
        print_status $YELLOW "üìã Check detailed logs and reports for more information"
    fi
    
    exit $exit_code
}

# Handle script arguments
case "${1:-}" in
    --help|-h)
        echo "Usage: $0 [OPTIONS]"
        echo ""
        echo "Run comprehensive final integration testing for LightRAG integration"
        echo ""
        echo "Options:"
        echo "  --help, -h     Show this help message"
        echo "  --dry-run      Show what would be executed without running tests"
        echo "  --quick        Run only critical tests (faster execution)"
        echo ""
        echo "Environment Variables:"
        echo "  DATABASE_URL      PostgreSQL database connection string"
        echo "  NEO4J_PASSWORD    Neo4j database password"
        echo "  GROQ_API_KEY      Groq API key for LLM access"
        echo ""
        echo "Output:"
        echo "  Test reports will be saved to: $REPORT_DIR"
        echo "  Execution log will be saved to: $LOG_FILE"
        exit 0
        ;;
    --dry-run)
        print_header "DRY RUN MODE"
        print_status $BLUE "This is what would be executed:"
        print_status $BLUE "1. Check prerequisites"
        print_status $BLUE "2. Install dependencies"
        print_status $BLUE "3. Run system readiness validation"
        print_status $BLUE "4. Run individual test suites"
        print_status $BLUE "5. Run final integration tests"
        print_status $BLUE "6. Generate summary reports"
        exit 0
        ;;
    --quick)
        print_status $YELLOW "‚ö° Quick mode: Running only critical tests"
        # Set flag for quick mode (would modify test execution)
        export QUICK_MODE=1
        ;;
esac

# Run main function
main "$@"